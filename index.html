<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robot Learning Symposium Karlsruhe</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: hsl(0, 0%, 98%);

        }
        .header {
            position: relative;
            background-image: url('images/image.png'); /* Replace with your image URL */
            background-size: cover;
            background-position: center;
            color: #fff;
            text-align: center;
            padding: 100px 20px;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */
        }
        
        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.4); /* Dark overlay for better text contrast */
            z-index: 1;
                        font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header h1 {
            position: relative;
            font-size: 2.8em;
            margin: 0;
            color: #ffffff;
            font-weight: 700;
            z-index: 2;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header .location {
            position: relative;
            font-size: 1.2em;
            color: #e0e0e0;
            margin-top: 10px;
            font-weight: 400;
            z-index: 2;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header p {
            position: relative;
            max-width: 600px;
            margin: 20px auto 0;
            font-size: 1.2em;
            color: #f0f0f0;
            line-height: 1.5;
            font-weight: 300;
            z-index: 2;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        @media (max-width: 768px) {
            .header {
                padding: 60px 20px;
            }
            .header h1 {
                font-size: 2em;
            }
            .header .location, .header p {
                font-size: 1em;
            }
        }
        .content {
            padding: 20px;
            background-color: #f9f9f9;
        }
        .content h2 {
            text-align: center;
            color: #004c3f;
        }
        .schedule {
            max-width: 800px;
            margin: 0 auto;
        }
        .schedule-item {
            display: flex;
            justify-content: space-between;
            align-items: center; /* Center vertically */
            padding: 10px 0;
            border-bottom: 1px solid #ccc;
        }
        .schedule-item:last-child {
            border-bottom: none;
        }
        .time {
            color: #666;
            width: 100px;
            text-align: right;
            margin: 10px;
            margin-right: 20px;
        }
        .event {
            width: calc(100% - 100px);
        }
        .event-title {
            font-weight: bold;
            margin: 0;
        }
        .speaker {
            color: #004c3f; /* Darker green to match the theme */
            font-size: 1em; /* Slightly larger font */
            font-weight: bold; /* Bolder font for emphasis */
            margin: 5px 0; /* Add spacing around the speaker name */
        }

        .speakers {
    max-width: 1000px;
    margin: 0 auto;
    padding: 20px 0;
}

.speaker-item {
    display: flex;
    align-items: flex-start;
    margin-bottom: 60px;
    flex-wrap: wrap; /* Allows items to wrap on smaller screens */
}

.speaker-image {
    width: 25%; /* Default size as a percentage */
    max-width: 200px; 
    height: auto; /* Keeps aspect ratio */
    border-radius: 10%;
    margin-right: 2%; /* Adds space between image and text */
    object-fit: cover;
    flex-shrink: 0;
}

.speaker-info {
    width: 70%; /* Takes the remaining space next to the image */
    max-width: 800px;
}
/* Responsive adjustments for smaller screens */
@media (max-width: 768px) {
    .speaker-item {
        flex-direction: column; /* Stack items vertically */
        align-items: center; /* Center-align items on small screens */
        text-align: center;
    }

    .speaker-image {
        width: 50%; /* Adjust image width to half of the container on small screens */
        max-width: 150px; /* Limit max width */
        margin-right: 0; /* Remove right margin since items are stacked */
        margin-bottom: 15px; /* Add spacing below the image */
    }

    .speaker-info {
        width: 90%; /* Make text take up most of the width */
        max-width: 100%; /* Allow text to take full width on small screens */
    }
}


.speaker-name {
    font-size: 1.4em;
    font-weight: bold;
    color: #004c3f;
    margin: 0;
}

.speaker-title {
    font-size: 1.1em;
    color: #333;
    margin-top: 5px;
}

.speaker-bio {
    color: #666;
    font-size: 0.9em;
    margin-top: 10px;

}
.abstract {
    color: #333;
    font-size: 0.9em;
    margin-top: 5px;
}
.talk-title {
    font-size: 1.1em;
    font-weight: bold;
    color: #333;
    margin: 3px 0; /* Adds spacing between title and speaker */
}


.white {
  color: #ffffff;
  font-family: Lato, sans-serif;
}

    </style>

</head>
<body>

<div class="header">
    <h1 class="white">Robot Learning Symposium</h1>
    <p class="location">Informatikom Karlsruhe, Atrium
       <br> Adenauerring 12, Karlsruhe Institute of Technology
        <br>November 5, 2024
    </p> <!-- Location line -->
    <p class="white">
        Current robotic systems have fallen short of public expectations, with most deployed solutions still operating within confined behavioral boundaries. The transition toward robots functioning seamlessly in everyday environments presents three key challenges that our symposium will explore: enabling flexible task execution through multimodal interactions, developing computational frameworks that mirror human cognitive flexibility, and creating systems that continuously refine their skills while maintaining operational safety. 
        
        <br><br>Our speakers will showcase pioneering approaches that blend self-supervised learning, large language models, and adaptive control strategies to address these fundamental challenges in modern robotics.
    </p>
</div>

<div class="content">
  <h2>Tentative schedule</h2>
  <div class="schedule">
      <div class="schedule-item">
          <div class="time">10:00</div>
          <div class="event">
              <p class="event-title">Welcome</p>
          </div>
      </div>

      <div class="schedule-item">
        <div class="time">10:10</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Embodied Multimodal Intelligence with Foundation Models</p>
            <p class="speaker">Oier Mees (UC Berkeley)</p>
            <p class="abstract">
              Despite considerable progress in robot learning and contrary to the expectations of the general public, the vast majority of robots deployed out in the real world today continue to remain restricted to a narrow set of preprogrammed behaviors for specific tasks. As robots become ubiquitous across human-centred environments, the need for "generalist" robots grows: how can we scale robot learning systems to generalize and adapt, allowing them to perform a wide range of everyday tasks in unstructured environments based on arbitrary, multimodal instructions from the users? In my work, I have focused on addressing the challenging problems of relating human language to a robot's multimodal perceptions and actions by introducing techniques that leverage self-supervision from uncurated data and common sense reasoning from foundation models from and for robotics.
            </p>
        </div>
    </div>

      <div class="schedule-item">
        <div class="time">10:50</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Systems 1 and 2 for Robot Learning</p>
            <p class="speaker">Katerina Fragkiadaki (CMU)</p>
            <p class="abstract">
              Humans can successfully handle both mundane and new and rare tasks simply by thinking harder and being more focused. How can we develop robots that think harder and do better in out-of-distribution scenarios? 
In this talk, we will marry today's generative models and traditional evolutionary search to enable better generalization of robot policies, and the ability to test-time reason through difficult scenarios, akin to a robot system 2. We will discuss learning behaviours from videos with 3D video perception as well as through language instructions and corrections that shape the robots' reward functions on-the-fly, and help us automate robot training data collection in simulators and in the real world. We will also discuss the development of better and faster simulators as universal data engines for robotics.
        </div>
    </div>

      <div class="schedule-item">
          <div class="time">11:40</div>
          <div class="event">
              <p class="event-title">Lunch Break</p>
          </div>
      </div>
      <div class="schedule-item">
          <div class="time">13:00</div>
          <div class="event">
              <p class="event-title">Poster Session</p>
              <p class="speaker">Robot Learning at KIT</p>
          </div>
      </div>

      <div class="schedule-item">
        <div class="time">14:00</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Safe and Robust Real-World Robot Learning</p>
            <p class="speaker">Davide Tateo (TU Darmstadt)</p>
            <p class="abstract">
              Nowadays, it is clear that we need to incorporate learning methods to develop the robots of the future. However, learning is not enough to empower the robot to deal with challenging real-world scenarios: we need to enable the robots to adapt dynamically to the environment with online learning. To allow online learning in real robotic systems, we need to solve three key challenges: efficient learning, robustness to disturbances, and satisfaction of safety constraints. In this talk, we will discuss these challenges and show how to deploy learning methods in complex contact-rich dynamic tasks such as the robot Air Hockey setting.
          </p>
        </div>
    </div>

      <div class="schedule-item">
          <div class="time">14:40</div>
          <div class="event">
              <p class="event-title">Final Remarks</p>
          </div>
      </div>
  </div>



  <hr style="margin: 3% 0; border: 0; border-top: 4px solid #ccc;">

    <h2>Speakers</h2>
    <div class="speakers">


        <div class="speaker-item">
            <img src="images/fragkiadaki.png" alt="Katerina Fragkiadaki" class="speaker-image">
            <div class="speaker-info">
                <p class="speaker-name">Katerina Fragkiadaki</p>
                <!-- <p class="speaker-title">Title</p> -->
                <p class="speaker-bio">
                  Katerina Fragkiadaki is the JPMorgan Chase Associate Professor in the Machine
  Learning Department in Carnegie Mellon University. She received her undergraduate diploma from Electrical and Computer Engineering in the National Technical University of Athens. 
  She received her Ph.D. from University of Pennsylvania and was a postdoctoral fellow in
  UC Berkeley and Google research after that.  Her work focuses on combining
  forms of common sense reasoning, such as spatial understanding and 3D scene understanding, with deep visuomotor learning.  The goal of her work is to enable few-shot learning and continual learning for perception, action and language grounding.  Her  group develops methods for computer vision for mobile agents, 2D and 3D visual parsing, 2D-to-3D perception,  vision-language grounding,  learning of object dynamics, navigation and manipulation policies. Pioneering innovations of her group's research include 2D-to-3D geometry-aware neural networks for 3D understanding from 2D video streams,   analogy-forming networks for memory-augmented few-shot visual parsing, and language-grounding in 2D and 3D scenes with bottom-up and top-down attention.  Her work has been awarded with a best Ph.D. thesis award,  an NSF CAREER award, AFOSR Young Investigator award, a DARPA Young Investigator award, Google, TRI, Amazon, UPMC and Sony faculty research awards. She is a program chair for ICLR 2024.
                </p>
            </div>
        </div>

        <div class="speaker-item">
            <img src="images/tateo.png" alt="Davide Tateo" class="speaker-image">
            <div class="speaker-info">
                <p class="speaker-name">Davide Tateo</p>
                <!-- <p class="speaker-title">Title</p> -->
                <p class="speaker-bio">
                    Davide Tateo is a Research Group Leader at the Intelligent Autonomous Systems Laboratory in the Computer Science Department of the Technical University of Darmstadt.  He received his M.Sc. degree in Computer Engineering at Politecnico di Milano in 2014 and his Ph.D. in Information Technology from the same university in 2019.  Davide Tateo worked in many areas of Robotics and Reinforcement Learning, Planning, and Perception. His main research interest is Robot Learning, focusing on high-speed motions, locomotion, and safety.
                </p>
            </div>
        </div>
        <div class="speaker-item">
            <img src="images/mees.jpg" alt="Oier Mees" class="speaker-image">
            <div class="speaker-info">
                <p class="speaker-name">Oier Mees</p>
                <!-- <p class="speaker-title">Embodied Multimodal Intelligence with Foundation Models</p> -->
                <p class="speaker-bio">
                    Oier Mees is a PostDoc at UC Berkeley working with Prof. Sergey Levine. He received his PhD in Computer Science (summa cum laude) in 2023 from the Freiburg University supervised by Prof. Dr. Wolfram Burgard. His research focuses on robot learning, with the goal of enabling robots to intelligently interact with both the physical world and humans, and improve themselves over time. Concretely, he is interested in how we can build self-improving embodied foundation models that can generalize the same way humans do. His research has been nominated for (and received) several Best Paper Awards, including ICRA and RA-L. Previously, he also spent time at NVIDIA AI interning with Dieter Fox.
                </p>
            </div>
        </div>

</div>



</body>
</html>
