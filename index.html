<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>International Robot Learning Symposium Seoul</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: hsl(0, 0%, 98%);

        }
        .header {
            position: relative;
            background-image: url('images/image.png'); /* Replace with your image URL */
            background-size: cover;
            background-position: center;
            color: #fff;
            text-align: center;
            padding: 100px 20px;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */
        }
        
        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.4); /* Dark overlay for better text contrast */
            z-index: 1;
                        font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header h1 {
            position: relative;
            font-size: 2.8em;
            margin: 0;
            color: #ffffff;
            font-weight: 700;
            z-index: 2;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header .location {
            position: relative;
            font-size: 1.2em;
            color: #e0e0e0;
            margin-top: 10px;
            font-weight: 400;
            z-index: 2;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        .header p {
            position: relative;
            max-width: 600px;
            margin: 20px auto 0;
            font-size: 1.2em;
            color: #f0f0f0;
            line-height: 1.5;
            font-weight: 300;
            z-index: 2;
            font-family: 'Roboto', sans-serif; /* Apply Roboto font */

        }
        
        @media (max-width: 768px) {
            .header {
                padding: 60px 20px;
            }
            .header h1 {
                font-size: 2em;
            }
            .header .location, .header p {
                font-size: 1em;
            }
        }
        .content {
            padding: 20px;
            background-color: #f9f9f9;
        }
        .content h2 {
            text-align: center;
            color: #004c3f;
        }
        .schedule {
            max-width: 800px;
            margin: 0 auto;
        }
        .schedule-item {
            display: flex;
            justify-content: space-between;
            align-items: center; /* Center vertically */
            padding: 10px 0;
            border-bottom: 1px solid #ccc;
        }
        .schedule-item:last-child {
            border-bottom: none;
        }
        .time {
            color: #666;
            width: 100px;
            text-align: right;
            margin: 10px;
            margin-right: 20px;
        }
        .event {
            width: calc(100% - 100px);
        }
        .event-title {
            font-weight: bold;
            margin: 0;
        }
        .speaker {
            color: #004c3f; /* Darker green to match the theme */
            font-size: 1em; /* Slightly larger font */
            font-weight: bold; /* Bolder font for emphasis */
            margin: 5px 0; /* Add spacing around the speaker name */
        }

        .speakers {
    max-width: 1000px;
    margin: 0 auto;
    padding: 20px 0;
}

.speaker-item {
    display: flex;
    align-items: center; /* Vertically center image with heading/content */
    margin-bottom: 60px;
    flex-wrap: wrap; /* Allows items to wrap on smaller screens */
}

.speaker-image {
    width: 25%; /* Default size as a percentage */
    max-width: 200px; 
    height: auto; /* Keeps aspect ratio */
    border-radius: 10%;
    margin-right: 2%; /* Adds space between image and text */
    object-fit: cover;
    flex-shrink: 0;
    display: block; /* Allows horizontal centering with auto margins when needed */
}

.speaker-info {
    width: 70%; /* Takes the remaining space next to the image */
    max-width: 800px;
}
/* Responsive adjustments for smaller screens */
@media (max-width: 768px) {
    .speaker-item {
        flex-direction: column; /* Stack items vertically */
        align-items: center; /* Center-align items on small screens */
        text-align: center;
    }

    .speaker-image {
        width: 50%; /* Adjust image width to half of the container on small screens */
        max-width: 150px; /* Limit max width */
        margin: 0 auto 15px; /* Center horizontally and add spacing below */
    }

    .speaker-info {
        width: 90%; /* Make text take up most of the width */
        max-width: 100%; /* Allow text to take full width on small screens */
    }
}


.speaker-name {
    font-size: 1.4em;
    font-weight: bold;
    color: #004c3f;
    margin: 0;
}

.speaker-title {
    font-size: 1.1em;
    color: #333;
    margin-top: 5px;
}

.speaker-bio {
    color: #666;
    font-size: 0.9em;
    margin-top: 10px;

}
.abstract {
    color: #333;
    font-size: 0.9em;
    margin-top: 5px;
}
.talk-title {
    font-size: 1.1em;
    font-weight: bold;
    color: #333;
    margin: 3px 0; /* Adds spacing between title and speaker */
}


.white {
  color: #ffffff;
  font-family: Lato, sans-serif;
}

    </style>

</head>
<body>

<div class="header">
    <h1 class="white">Internetional Robot Learning Symposium Seoul</h1>
    <p class="location">
       Artificial Intelligence Institute Seoul National University (AIIS) <br>
       1 Gwanak-ro, Gwanak-gu, Seoul 08826 <br>
       Building 303, 2nd Floor <br>
        <br>September 26, 2025
    </p> <!-- Location line -->
    <p class="white">
        Current robotic systems have fallen short of public expectations, with most deployed solutions still operating within confined behavioral boundaries. The transition toward robots functioning seamlessly in everyday environments presents three key challenges that our symposium will explore: enabling flexible task execution through multimodal interactions, developing computational frameworks that mirror human cognitive flexibility, and creating systems that continuously refine their skills while maintaining operational safety. 
        
        <br><br>
        In this third edition of the international robot learning symposium, we expand our focus: from tactile sensing and contact-rich manipulation, to scalable humanoid vision-language-action models, to precise and constraint-aware robot behaviors. 
        Our speakers will showcase pioneering approaches that blend self-supervised learning, large language models, adaptive control strategies, high-resolution tactile sensing, and constraint-based planning to address these fundamental challenges in modern robotics.
    </p>
</div>

<div class="content">
  <h2>Tentative schedule</h2>
  <div style="text-align: center; font-style: italic; color: #666;">
    (The detailed schedule will be updated soon)
  </div>
<!--- 
  <div class="schedule">
      <div class="schedule-item">
          <div class="time">10:00</div>
          <div class="event">
              <p class="event-title">Welcome</p>
          </div>
      </div>

      <div class="schedule-item">
        <div class="time">10:10</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Embodied Multimodal Intelligence with Foundation Models</p>
            <p class="speaker">Oier Mees (UC Berkeley)</p>
            <p class="abstract">
              Despite considerable progress in robot learning and contrary to the expectations of the general public, the vast majority of robots deployed out in the real world today continue to remain restricted to a narrow set of preprogrammed behaviors for specific tasks. As robots become ubiquitous across human-centred environments, the need for "generalist" robots grows: how can we scale robot learning systems to generalize and adapt, allowing them to perform a wide range of everyday tasks in unstructured environments based on arbitrary, multimodal instructions from the users? In my work, I have focused on addressing the challenging problems of relating human language to a robot's multimodal perceptions and actions by introducing techniques that leverage self-supervision from uncurated data and common sense reasoning from foundation models from and for robotics.
            </p>
        </div>
    </div>

      <div class="schedule-item">
        <div class="time">10:50</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Systems 1 and 2 for Robot Learning</p>
            <p class="speaker">Katerina Fragkiadaki (CMU)</p>
            <p class="abstract">
              Humans can successfully handle both mundane and new and rare tasks simply by thinking harder and being more focused. How can we develop robots that think harder and do better in out-of-distribution scenarios? 
In this talk, we will marry today's generative models and traditional evolutionary search to enable better generalization of robot policies, and the ability to test-time reason through difficult scenarios, akin to a robot system 2. We will discuss learning behaviours from videos with 3D video perception as well as through language instructions and corrections that shape the robots' reward functions on-the-fly, and help us automate robot training data collection in simulators and in the real world. We will also discuss the development of better and faster simulators as universal data engines for robotics.
        </div>
    </div>

      <div class="schedule-item">
          <div class="time">11:40</div>
          <div class="event">
              <p class="event-title">Lunch Break</p>
          </div>
      </div>
      <div class="schedule-item">
          <div class="time">13:00</div>
          <div class="event">
              <p class="event-title">Poster Session</p>
              <p class="speaker">Robot Learning at KIT</p>
          </div>
      </div>

      <div class="schedule-item">
        <div class="time">14:00</div>
        <div class="event">
            <p class="event-title">Talk</p>
            <p class="talk-title">Safe and Robust Real-World Robot Learning</p>
            <p class="speaker">Davide Tateo (TU Darmstadt)</p>
            <p class="abstract">
              Nowadays, it is clear that we need to incorporate learning methods to develop the robots of the future. However, learning is not enough to empower the robot to deal with challenging real-world scenarios: we need to enable the robots to adapt dynamically to the environment with online learning. To allow online learning in real robotic systems, we need to solve three key challenges: efficient learning, robustness to disturbances, and satisfaction of safety constraints. In this talk, we will discuss these challenges and show how to deploy learning methods in complex contact-rich dynamic tasks such as the robot Air Hockey setting.
          </p>
        </div>
    </div>

      <div class="schedule-item">
          <div class="time">14:40</div>
          <div class="event">
              <p class="event-title">Final Remarks</p>
          </div>
      </div>
  </div>

--->

  <hr style="margin: 3% 0; border: 0; border-top: 4px solid #ccc;">


  <h2>Speakers</h2>
  <div class="speakers">

      <!-- Existing Speakers remain -->

      <!-- New Speakers -->
      <div class="speaker-item">
          <img src="images/fazeli.webp" alt="Nima Fazeli" class="speaker-image">
          <div class="speaker-info">
              <p class="speaker-name">Nima Fazeli</p>
              <p class="speaker-bio">
                 Nima Fazeli is an Assistant Professor of Robotics, Computer Science (EECS), and Mechanical Engineering at the University of Michigan, and an Amazon Scholar with Amazon Robotics. He leads the Manipulation and Machine Intelligence (MMint) Lab, which focuses on intelligent and dexterous robotic manipulation through advances in sensing, learning, and control. Nima received his Ph.D. from MIT in 2019, where he worked with Prof. Alberto Rodriguez. He earned his M.Sc. from the University of Maryland, College Park in 2014, where his research focused on modeling the human (and occasionally swine) arterial tree for applications in cardiovascular disease, diabetes, and cancer diagnosis. His work has been recognized with the NSF CAREER Award, support from the National Robotics Initiative and NSF Advanced Manufacturing, and the Rohsenow Fellowship. His research has also been featured in major media outlets including The New York Times, CBS, CNN, and BBC.
              </p>
          </div>
      </div>

      <div class="speaker-item">
          <img src="images/khadiv.jpeg" alt="Majid Khadiv" class="speaker-image">
          <div class="speaker-info">
              <p class="speaker-name">Majid Khadiv</p>
              <p class="speaker-bio">
Majid Khadiv is an assistant professor in the school of Computation, Information and Technology (CIT) at TUM. He leads the chair of AI Planning in Dynamic Environments and is also a member of the Munich Institute of Robotics and Machine Intelligence (MIRMI). Prior to joining TUM, he was a research scientist at the Empirical Inference Department at the Max Planck Institute for Intelligent systems. Before that he was a postdoctoral researcher in the Machines in Motion, a joint laboratory between New York University and Max Planck Institute. Since the start of his PhD in 2012, he has been performing research on motion planning, control and learning for legged robots ranging from quadrupeds, lower-limb exoskeleton up to humanoid robots.

              </p>
          </div>
      </div>

      <div class="speaker-item">
          <img src="images/park.jpg" alt="Daehyung Park" class="speaker-image">
          <div class="speaker-info">
              <p class="speaker-name">Daehyung Park</p>
              <p class="speaker-bio">
                 Daehyung Park is an associate professor at the School of Computing, KAIST, Korea, leading the Robust Intelligence and Robotics Laboratory (RIRO Lab). His research lies at the intersection of mobile manipulation, artificial intelligence, and human-robot interaction to advance collaborative robot technologies. Prior to joining KAIST, he had been a postdoctoral associate in Computer Science and Artificial Intelligence Laboratory (CSAIL) at Massachusetts Institute of Technology (MIT), working on inverse constraint learning and semantic knowledge estimation. He received a Ph.D. in Robotics at Georgia Institute of Technology, an M.S. from the University of Southern California, and a B.S. from Osaka University. Prior to joining his Ph.D., he served as a Robotics Researcher at Samsung Electronics Inc from 2008-2012. His work was awarded the Outstanding Planning Paper Award at ICRA 2023, Google Research Scholar Award 2022, an Outstanding Navigation Award Finalist at ICRA 2022, etc. See more at:  <a href="https://rirolab.kaist.ac.kr/">https://rirolab.kaist.ac.kr/</a>
      </div>

  </div>



</body>
</html>
